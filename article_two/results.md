## mistral-small-3.2. ##
--- Final Results ---
Comparing the average scores from both prompts:

| Prompt Style |Avg ROUGE-1 | Avg BERTScore (F1) |
| -------- | -------: | -------: | 
| Vague 'Lennie' Prompt | 0.505624 | 0.902519 |
| Specific 'George' Prompt | 0.662811 | 0.904781 |

Experiment complete. As the data shows, the specific 'George' prompt consistently yields higher scores.

| Prompt Style |Avg ROUGE-1 | Avg BERTScore (F1) |
| -------- | -------: | -------: | 
| Vague 'Lennie' Prompt |    0.509366 | 0.899178 |
| Specific 'George' Prompt | 0.659506 | 0.904045 |

## Gemma 3 27B Instruct Q3_K_L ##

Interestingly Gemma 3 27B Instruct Q3_K_L would sometimes have Lennie outperform george.

--- Final Results ---
Comparing the average scores from both prompts:
| Prompt Style |Avg ROUGE-1 | Avg BERTScore (F1) |
| -------- | -------: | -------: | 
| Vague 'Lennie' Prompt | 0.535427 | 0.886157 |
| Specific 'George' Prompt | 0.541568 | 0.888456 |

--- Final Results ---
Comparing the average scores from both prompts:
| Prompt Style |Avg ROUGE-1 | Avg BERTScore (F1) |
| -------- | -------: | -------: | 
| Vague 'Lennie' Prompt | 0.517007 | 0.886884 |
| Specific 'George' Prompt | 0.534392 | 0.884203 |
